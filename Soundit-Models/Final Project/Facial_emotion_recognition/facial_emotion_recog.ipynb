{"cells":[{"cell_type":"markdown","metadata":{"id":"PJPo8IwUvDCs"},"source":["# **Setup**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6680,"status":"ok","timestamp":1623834736568,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"TF6tceinbqKY","outputId":"da243908-71a8-4158-d934-60cba7502d6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0Metal device set to: Apple M1 Max\n","\n","systemMemory: 32.00 GB\n","maxCacheSize: 10.67 GB\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["2023-04-09 11:41:07.899800: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2023-04-09 11:41:07.899898: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1623837825671,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"VPlyEm18uWGc"},"outputs":[{"name":"stderr","output_type":"stream","text":["objc[14324]: Class CaptureDelegate is implemented in both /Users/kevynkrancenblum/.local/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x2bcf6e4e8) and /Users/kevynkrancenblum/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x2be930860). One of the two will be used. Which one is undefined.\n","objc[14324]: Class CVWindow is implemented in both /Users/kevynkrancenblum/.local/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x2bcf6e538) and /Users/kevynkrancenblum/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x29d17ca68). One of the two will be used. Which one is undefined.\n","objc[14324]: Class CVView is implemented in both /Users/kevynkrancenblum/.local/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x2bcf6e560) and /Users/kevynkrancenblum/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x29d17ca90). One of the two will be used. Which one is undefined.\n","objc[14324]: Class CVSlider is implemented in both /Users/kevynkrancenblum/.local/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x2bcf6e588) and /Users/kevynkrancenblum/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x29d17cab8). One of the two will be used. Which one is undefined.\n"]}],"source":["import numpy as np\n","import cv2\n","import mediapipe as mp\n","import time\n","import glob\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"pa3K5S8eb1XO"},"source":["# **Parameters & Model**"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1623834857084,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"E2wWgnggQOpk"},"outputs":[],"source":["emotions = {\n","    0: ['Angry', (0,0,255), (255,255,255)],\n","    1: ['Disgust', (0,102,0), (255,255,255)],\n","    2: ['Fear', (255,255,153), (0,51,51)],\n","    3: ['Happy', (153,0,153), (255,255,255)],\n","    4: ['Sad', (255,0,0), (255,255,255)],\n","    5: ['Surprise', (0,255,0), (255,255,255)],\n","    6: ['Neutral', (160,160,160), (255,255,255)]\n","}\n","num_classes = len(emotions)\n","input_shape = (48, 48, 1)\n","weights_1 = 'saved_models/vggnet.h5'\n","weights_2 = 'saved_models/vggnet_up.h5'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1623834860705,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"2JaSYECtFIbP"},"outputs":[],"source":["class VGGNet(Sequential):\n","    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n","        super().__init__()\n","        self.add(Rescaling(1./255, input_shape=input_shape))\n","        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n","        self.add(BatchNormalization())\n","        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(MaxPool2D())\n","        self.add(Dropout(0.5))\n","\n","        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(MaxPool2D())\n","        self.add(Dropout(0.4))\n","\n","        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(MaxPool2D())\n","        self.add(Dropout(0.5))\n","\n","        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n","        self.add(BatchNormalization())\n","        self.add(MaxPool2D())\n","        self.add(Dropout(0.4))\n","\n","        self.add(Flatten())\n","        \n","        self.add(Dense(1024, activation='relu'))\n","        self.add(Dropout(0.5))\n","        self.add(Dense(256, activation='relu'))\n","\n","        self.add(Dense(num_classes, activation='softmax'))\n","\n","        self.compile(optimizer=Adam(learning_rate=lr),\n","                    loss=categorical_crossentropy,\n","                    metrics=['accuracy'])\n","        \n","        self.checkpoint_path = checkpoint_path"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":865,"status":"ok","timestamp":1623834866947,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"s1N9r5H2iKEt"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-09 11:41:14.450422: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2023-04-09 11:41:14.450447: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["model_1 = VGGNet(input_shape, num_classes, weights_1)\n","model_1.load_weights(model_1.checkpoint_path)\n","\n","model_2 = VGGNet(input_shape, num_classes, weights_2)\n","model_2.load_weights(model_2.checkpoint_path)"]},{"cell_type":"markdown","metadata":{"id":"tk4W6vYxcdBE"},"source":["# **Inference**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1623834879548,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"O2o4dgiIRTr2"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"]}],"source":["mp_face_detection = mp.solutions.face_detection\n","mp_drawing = mp.solutions.drawing_utils\n","face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.6)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"non-default argument follows default argument (3900953969.py, line 15)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[7], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_bbox(landmarks, scale=1, image):\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"]}],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","\n","# Initialize MediaPipe FaceMesh and Drawing utilities\n","mp_face_mesh = mp.solutions.face_mesh\n","mp_drawing = mp.solutions.drawing_utils\n","face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,max_num_faces=1,refine_landmarks=True,min_detection_confidence=0.5)\n","\n","# Function to draw a bounding box\n","def draw_bbox(image, bbox, color=(0, 255, 0), thickness=2):\n","    cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, thickness)\n","\n","# Function to calculate bounding box from landmarks\n","def get_bbox(landmarks, scale=1, image):\n","    coords = np.array([[lmk.x, lmk.y] for lmk in landmarks.landmark])\n","    min_coords = np.min(coords, axis=0)\n","    max_coords = np.max(coords, axis=0)\n","    bbox = np.hstack([min_coords, max_coords])\n","    bbox = np.int32(bbox * [image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n","    return bbox\n","\n","# Start the webcam capture\n","cap = cv2.VideoCapture(0)\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Convert the frame to RGB and process with MediaPipe\n","    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    results = face_mesh.process(frame_rgb)\n","\n","    # Draw the bounding box for the first face detected\n","    if results.multi_face_landmarks:\n","        bbox = get_bbox(results.multi_face_landmarks[0],frame)\n","        draw_bbox(frame, bbox)\n","\n","    # Show the frame\n","    cv2.imshow(\"MediaPipe FaceMesh\", frame)\n","\n","    # Exit if the user presses the 'q' key\n","    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","        break\n","\n","# Release the webcam and close the display window\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":387,"status":"ok","timestamp":1623836538326,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"QxxLhqRwz0__"},"outputs":[],"source":["def detection_preprocessing(image, h_max=360):\n","    h, w, _ = image.shape\n","    if h > h_max:\n","        ratio = h_max / h\n","        w_ = int(w * ratio)\n","        image = cv2.resize(image, (w_,h_max))\n","    return image\n","\n","def resize_face(face):\n","    x = tf.expand_dims(tf.convert_to_tensor(face), axis=2)\n","    return tf.image.resize(x, (48,48))\n","\n","def recognition_preprocessing(faces):\n","    x = tf.convert_to_tensor([resize_face(f) for f in faces])\n","    return x"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":351,"status":"ok","timestamp":1623836738256,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"VT9FGSGDz2hr"},"outputs":[],"source":["resultat=[]\n","def inference(image):\n","    H, W, _ = image.shape\n","    \n","    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    results = face_detection.process(rgb_image)\n","\n","    if results.detections:\n","        faces = []\n","        pos = []\n","        for detection in results.detections:\n","            box = detection.location_data.relative_bounding_box\n","            mp_drawing.draw_detection(image, detection)\n","\n","            x = int(box.xmin * W)\n","            y = int(box.ymin * H)\n","            w = int(box.width * W)\n","            h = int(box.height * H)\n","\n","            x1 = max(0, x)\n","            y1 = max(0, y)\n","            x2 = min(x + w, W)\n","            y2 = min(y + h, H)\n","\n","            face = image[y1:y2,x1:x2]\n","            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n","            faces.append(face)\n","            pos.append((x1, y1, x2, y2))\n","    \n","        x = recognition_preprocessing(faces)\n","\n","        y_1 = model_1.predict(x)\n","        y_2 = model_2.predict(x)\n","        l = np.argmax(y_1+y_2, axis=1)\n","        \n","        for i in range(len(faces)):\n","            cv2.rectangle(image, (pos[i][0],pos[i][1]),\n","                            (pos[i][2],pos[i][3]), emotions[l[i]][1], 2, lineType=cv2.LINE_AA)\n","            cv2.rectangle(image, (pos[i][0],pos[i][1]-20),\n","                            (pos[i][2]+20,pos[i][1]), emotions[l[i]][1], -1, lineType=cv2.LINE_AA)\n","            \n","            cv2.putText(image, f'{emotions[l[i]][0]}', (pos[i][0],pos[i][1]-5),\n","                            0, 0.6, emotions[l[i]][2], 2, lineType=cv2.LINE_AA)\n","\n","            resultat.append(emotions[l[i]][0])    \n","    return image"]},{"cell_type":"markdown","metadata":{"id":"hRE0GhXpbUkM"},"source":["## **Video TESTING**"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1623833509820,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"ekc0JiaQbT07"},"outputs":[],"source":["video = '/Users/kevynkrancenblum/Desktop/Data Science/Final Project/VideoBody/Happy/lironhappy1.MOV'\n","cap = cv2.VideoCapture(video)\n","frame_width = int(cap.get(3))\n","frame_height = int(cap.get(4))\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","target_h = 360\n","target_w = int(target_h * frame_width / frame_height)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":73186,"status":"ok","timestamp":1623833588636,"user":{"displayName":"Jianming Han","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggzk6L5tAZ5Gi8DaqgCsqP9WRLwiNEdgvr209DkQA=s64","userId":"00040175033446167208"},"user_tz":-120},"id":"Jcd5sst2iGp2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 14ms/step\n","1/1 [==============================] - 0s 14ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["while True:\n","    success, image = cap.read()\n","    if success:\n","        #image = resize.image(image)\n","        result = inference(image)\n","        cv2.imshow('Facial Emotion Recognition', result)\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            \n","            break\n","    else:\n","        break\n","    \n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"JJ8Xi3ETpYkX"},"source":["# **Cam**"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"em_EXSUnieJe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sad\n"]}],"source":["def most_frequent(List):\n","    counter = 0\n","    num = List[0]\n","     \n","    for i in List:\n","        curr_frequency = List.count(i)\n","        if(curr_frequency> counter):\n","            counter = curr_frequency\n","            num = i\n"," \n","    return num\n","print(most_frequent(resultat))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["uniques, counts = np.unique(resultat, return_counts=True)\n","percentages = dict(zip(uniques, counts * 100 / len(resultat)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'Angry': 45.45454545454545,\n"," 'Happy': 4.545454545454546,\n"," 'Neutral': 22.727272727272727,\n"," 'Sad': 27.272727272727273}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["percentages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"inference.ipynb","provenance":[{"file_id":"1dV-sPmPSOyCRsKqGRs7CK8V4h22ht42m","timestamp":1623601457273},{"file_id":"1muoOaBA21nwc1MUlwEDcVyqAuoa5_AYu","timestamp":1623599622276}]},"kernelspec":{"display_name":"Python 3.9.13 ('tensorflow')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"8f27e84c7fb4756fe30ebd2212452f90785c16891dd0ca65c023d02a7a434102"}}},"nbformat":4,"nbformat_minor":0}
